# -*- coding: utf-8 -*-
"""LLM Project_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xEEvrYzImkL_z-PUcR6Alr61nw-2DHFE
"""

!pip uninstall -y pyarrow -q
!rm -rf /usr/local/lib/python3.12/dist-packages/pyarrow*

# Core libraries (transformers, embeddings, FAISS, etc.)
!pip install -U \
    transformers \
    accelerate \
    bitsandbytes \
    sentence-transformers \
    faiss-cpu \
    datasets

# NLP / text utilities
!pip install textstat beautifulsoup4 lxml gTTS

!pip install pillow timm torch torchvision easyocr

# Core native deps for ocrmypdf
!apt-get -q update
!apt-get -q install -y ocrmypdf tesseract-ocr qpdf ghostscript pngquant poppler-utils
# Python libs we use elsewhere
!pip -q install pymupdf pdf2image easyocr pillow tqdm
!pip install -q pytesseract # Install pytesseract

import re, requests
from bs4 import BeautifulSoup
from typing import List, Dict

"""# Installations"""

import torch, platform, os, gc
print("Torch:", torch.__version__, "| CUDA available:", torch.cuda.is_available())

import re, json, time, requests
from typing import List, Dict
from bs4 import BeautifulSoup
import numpy as np

def clean_text(s: str) -> str:
    s = re.sub(r'\s+', ' ', s).strip()
    # collapse long lines a bit for readability
    return s

"""## Config: patient profile & project toggles"""

# Patient test profiles

patient_profiles = [
    # English, low literacy
    {"age": 65, "literacy": "low", "language": "English", "drug": "metoprolol"},
    {"age": 55, "literacy": "low", "language": "English", "drug": "lisinopril"},

    # English, high literacy
    {"age": 45, "literacy": "high", "language": "English", "drug": "amoxicillin"},

    # Spanish, low literacy
    {"age": 70, "literacy": "low", "language": "Spanish", "drug": "metoprolol"},
    {"age": 60, "literacy": "low", "language": "Spanish", "drug": "amoxicillin"},

    # Spanish, moderate literacy
    {"age": 50, "literacy": "moderate", "language": "Spanish", "drug": "lisinopril"},

    # English, very low literacy
    {"age": 80, "literacy": "very low", "language": "English", "drug": "metformin"},

    # Spanish, bilingual edge case
    {"age": 68, "literacy": "low", "language": "Spanish-English", "drug": "atorvastatin"},
]

# Retrieval settings (global)
TOP_K = 8
MIN_CHUNK_LEN = 200  # characters

"""## Open data loaders (MedlinePlus + openFDA)"""

# Fetch + Combine MedlinePlus and openFDA for ALL drugs


def fetch_medlineplus_drug(drug: str) -> Dict | None:
    """
    Fetches consumer-facing drug guidance from MedlinePlus based on drug name.
    Falls back to generic URLs if specific path not known.
    """
    drug_paths = {
        "metoprolol": "druginfo/meds/a682864.html",
        "lisinopril": "druginfo/meds/a692051.html",
        "amoxicillin": "druginfo/meds/a685001.html",
    }

    topic_or_url = drug_paths.get(drug.lower())
    if not topic_or_url:
        print(f"‚ö†Ô∏è No MedlinePlus path known for {drug.title()} ‚Äî skipping MedlinePlus fetch.")
        return None

    url = topic_or_url if topic_or_url.startswith("http") else f"https://medlineplus.gov/{topic_or_url}"

    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        paras = [p.get_text(" ", strip=True) for p in soup.select("div.section > p")]
        text = "\n".join(paras)
        return {"source": "MedlinePlus", "drug": drug, "url": url, "text": clean_text(text)}

    except Exception as e:
        print(f" Failed to fetch MedlinePlus data for {drug.title()}: {e}")
        return None


def fetch_openfda_label(drug: str) -> Dict | None:
    """
    Pulls FDA drug label JSON (DailyMed SPL) via openFDA.
    Returns a subset of sections useful for patient instructions.
    """
    api = "https://api.fda.gov/drug/label.json"
    q = f'openfda.brand_name:"{drug}" + openfda.generic_name:"{drug}"'
    try:
        r = requests.get(api, params={"search": q, "limit": 1}, timeout=30)
        j = r.json()
        if "results" not in j or not j["results"]:
            print(f"‚ö†Ô∏è openFDA label not found for {drug.title()}")
            return None
        doc = j["results"][0]
    except Exception as e:
        print(f" openFDA fetch failed for {drug.title()}: {e}")
        return None

    keep = [
        "indications_and_usage",
        "dosage_and_administration",
        "warnings",
        "patient_medication_information",
        "information_for_patients",
        "contraindications",
        "adverse_reactions"
    ]

    chunks = []
    for k in keep:
        if k in doc:
            v = doc[k]
            if isinstance(v, list):
                v = " ".join(v)
            chunks.append(f"## {k.replace('_', ' ').title()}\n{v}")

    text = "\n\n".join(chunks)
    return {"source": "openFDA", "drug": drug, "id": doc.get("id", ""), "text": clean_text(text)}


unique_drugs = sorted(set([p["drug"].lower() for p in patient_profiles]))
print(f"Fetching content for {len(unique_drugs)} drugs: {unique_drugs}")

corpora = []

for drug in unique_drugs:
    print(f"\nüîç Loading data for {drug.title()} ...")
    medline = fetch_medlineplus_drug(drug)
    openfda = fetch_openfda_label(drug)

    # merge only valid text sources
    sources = [c for c in [medline, openfda] if c and c.get("text")]
    corpora.extend(sources)

    print(" Loaded sources:", [c["source"] for c in sources],
          "| Total chars:", sum(len(c["text"]) for c in sources))
    if not openfda:
        print(f" Note: openFDA entry not found for {drug.title()} ‚Äî using MedlinePlus only.")

print(f"\n Final combined corpus: {len(corpora)} documents from {len(unique_drugs)} drugs.")

import pandas as pd
import re

def chunk_with_meta(corpus, min_len=MIN_CHUNK_LEN):
    """
    Splits each document in the corpus into manageable chunks,
    preserving metadata (source, section, and drug).
    Suitable for multi-drug RAG pipelines.
    """
    rows = []
    for c in corpus:
        src = c.get("source", "Unknown")
        text = c.get("text", "").strip()
        drug = c.get("drug", "Unknown").lower()

        if not text:
            continue  # skip empty docs

        # Split paragraphs on blank lines
        parts = re.split(r"\n{2,}", text)
        for p in parts:
            p = p.strip()
            if len(p) < min_len:
                continue

            # Try to extract heading-based section
            m = re.match(r"^##\s*([A-Za-z0-9 \-]+)\s*\n", p)
            section = m.group(1).strip() if m else (
                "General" if src == "MedlinePlus" else "Unlabeled"
            )

            rows.append({
                "drug": drug,
                "source": src,
                "section": section,
                "text": p[:2500],  # truncation safeguard
                "char_len": len(p),
                "word_len": len(p.split())
            })

    df = pd.DataFrame(rows)
    print(f" Created {len(df)} chunks from {df['drug'].nunique()} drugs, avg {df['char_len'].mean():.0f} chars per chunk.")
    return df

# Run chunking on the unified multi-drug corpus
chunk_df = chunk_with_meta(corpora)
chunk_df.head(5)

from sentence_transformers import SentenceTransformer

embedder = SentenceTransformer("BAAI/bge-small-en-v1.5")

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import matplotlib.pyplot as plt

def top_tfidf_terms(texts, k=12, stop_words="english"):
    vec = TfidfVectorizer(stop_words=stop_words, max_features=5000)
    M = vec.fit_transform(texts)
    means = np.asarray(M.mean(axis=0)).ravel()
    vocab = np.array(vec.get_feature_names_out())
    top_idx = means.argsort()[-k:][::-1]
    return list(zip(vocab[top_idx], means[top_idx]))

for src in chunk_df["source"].unique():
    src_texts = chunk_df.loc[chunk_df["source"]==src, "text"].tolist()
    tops = top_tfidf_terms(src_texts, k=12)
    terms, scores = zip(*tops)

    plt.figure(figsize=(7,3.5))
    y = np.arange(len(terms))[::-1]
    plt.barh(y, scores)
    plt.yticks(y, terms)
    plt.title(f"Top TF-IDF terms ‚Äî {src}")
    plt.xlabel("Mean TF-IDF")
    plt.tight_layout()
    plt.show()

!pip -q install wordcloud
from wordcloud import WordCloud

for src in chunk_df["source"].unique():
    text_all = " ".join(chunk_df.loc[chunk_df["source"]==src, "text"])
    wc = WordCloud(width=800, height=400, background_color="white",
                   stopwords="english", colormap="viridis").generate(text_all)
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Word Cloud ‚Äî {src}")
    plt.show()

from collections import Counter
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def get_top_ngrams(texts, n=2, k=10):
    tokens = []
    for t in texts:
        words = [w.lower() for w in re.findall(r"\b[a-zA-Z]{3,}\b", t) if w.lower() not in ENGLISH_STOP_WORDS]
        tokens.extend(zip(*[words[i:] for i in range(n)]))
    cnt = Counter(tokens)
    return cnt.most_common(k)

bigrams = get_top_ngrams(chunk_df["text"].tolist(), n=2, k=12)
labels = [" ".join(bg) for bg,_ in bigrams]
vals   = [c for _,c in bigrams]

plt.figure(figsize=(7,3.5))
plt.barh(np.arange(len(labels))[::-1], vals)
plt.yticks(np.arange(len(labels))[::-1], labels)
plt.title("Top Bigrams in Corpus")
plt.show()

"""## Embed & index with FAISS (BGE-small)"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
from typing import List, Dict

EMB_MODEL = "BAAI/bge-small-en-v1.5"
embedder = SentenceTransformer(EMB_MODEL)

def build_chunks(corpus: List[Dict], min_len=MIN_CHUNK_LEN) -> List[Dict]:
    """
    Builds chunks from the corpus while keeping drug and source metadata.
    """
    chunks = []
    for c in corpus:
        text = c.get("text", "").strip()
        if not text:
            continue
        drug = c.get("drug", "unknown").lower()
        src = c.get("source", "unknown")

        for para in re.split(r"\n{2,}", text):
            para = para.strip()
            if len(para) < min_len:
                continue
            chunks.append({
                "drug": drug,
                "source": src,
                "text": para[:2500]  # limit for token safety
            })
    print(f"Created {len(chunks)} chunks across {len(set(c['drug'] for c in chunks))} drugs.")
    return chunks


def build_index(chunks: List[Dict]):
    """
    Encodes text chunks and builds a FAISS index for retrieval.
    """
    texts = [c["text"] for c in chunks]
    X = embedder.encode(
        texts,
        normalize_embeddings=True,
        convert_to_numpy=True,
        show_progress_bar=True
    )
    index = faiss.IndexFlatIP(X.shape[1])
    index.add(X)
    print(f"FAISS index built with {len(texts)} chunks, dim={X.shape[1]}")
    return index, X


def retrieve(index, chunks: List[Dict], query: str, drug: str = None, k: int = TOP_K) -> str:
    """
    Retrieves the top-k relevant chunks for a given query.
    Optionally filters results by drug name.
    """
    qv = embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)
    D, I = index.search(qv, min(k, len(chunks)))

    results = [chunks[i] for i in I[0]]
    if drug:
        filtered = [r for r in results if r["drug"] == drug.lower()]
        if filtered:
            results = filtered

    ctx = "\n\n".join(f"[{r['drug'].title()} - {r['source']}]\n{r['text']}" for r in results)
    return ctx


chunks = build_chunks(corpora)
index, _ = build_index(chunks)
print(f"FAISS index ready with {len(chunks)} total chunks.")

# Example retrieval
query = "What is this medicine used for and how should I take it?"
context = retrieve(index, chunks, query, drug="metoprolol", k=TOP_K)
print("\n--- Retrieved Context ---\n")
print(context[:1000])

"""## Load open-source LLM (Qwen-2.5 1.5B, 4-bit) + grounded generation"""

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch, json, re

def load_llm():
    model_id = "Qwen/Qwen2.5-1.5B-Instruct"
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    mdl = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        quantization_config=quant_config
    )
    return pipeline(
        "text-generation",
        model=mdl,
        tokenizer=tok,
        max_new_tokens=350,
        do_sample=False,
        temperature=0.0,
        top_p=0.9,
        top_k=30,
        repetition_penalty=1.2,
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.eos_token_id
    )

gen = load_llm()

SYSTEM = (
    "You are a clinical patient-education assistant. "
    "Generate concise discharge instructions using ONLY the facts in [CONTEXT].\n\n"
    "RULES:\n"
    "- Do not invent or guess.\n"
    "- Do not include any unrelated or extra content.\n"
    "- Do not repeat the word 'End' more than once.\n"
    "- Keep each section under 3 short bullet points.\n"
    "- Always follow the section template exactly.\n"
    "- End the output immediately after the 'Summary' section.\n\n"
    "FORMAT:\n"
    "### Discharge Instructions\n"
    "**When to Take:**\n- ...\n\n"
    "**What to Avoid:**\n- ...\n\n"
    "**Common Side Effects:**\n- ...\n\n"
    "**When to Call a Doctor:**\n- ...\n\n"
    "**Summary:**\n- ...\n\n"
    "End with this exact line:\n"
    "'This information is based only on provided medical sources.'"
)

def clean_output(text):
    """Clean repeated endings and unrelated drift."""
    text = text.split("Human resources department")[0]  # stop HR hallucinations
    text = re.sub(r"(?i)(end\.?\s*){2,}", "End.", text)
    text = re.sub(r"(?i)(thank you|have a|good day|safe travels).*", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def make_leaflet(patient, context):
    user = (
        f"Patient info:\n"
        f"- Age: {patient['age']}\n"
        f"- Literacy: {patient['literacy']}\n"
        f"- Language: {patient['language']}\n"
        f"- Drug: {patient['drug']}\n"
        "Generate the leaflet in the language specified. Be brief, factual, and follow the format exactly."
    )
    prompt = f"[SYSTEM]\n{SYSTEM}\n\n[CONTEXT]\n{context}\n\n[USER]\n{user}\n[ASSISTANT]"
    out = gen(prompt, max_new_tokens=350, truncation=True)[0]["generated_text"]
    leaflet = out.split("[ASSISTANT]")[-1].strip()
    return clean_output(leaflet)

def summarize_leaflet(text):
    """Short factual summary only (2 sentences max)."""
    summary_prompt = (
        "Summarize the leaflet below in 2 short factual sentences. "
        "Do not repeat phrases, and end with a period.\n\n"
        f"{text}\n\nSummary:"
    )
    out = gen(summary_prompt, max_new_tokens=80, do_sample=False)[0]["generated_text"]
    summary = out.split("Summary:")[-1].strip()
    summary = re.sub(r"(?i)(end\.?\s*){1,}", "", summary)
    return summary.split("\n")[0].strip()

# Run across patients
results = []
for i, PATIENT in enumerate(patient_profiles, start=1):
    print(f"\n{'='*70}")
    print(f"Case {i}: {PATIENT['drug']} | {PATIENT['language']} | Literacy: {PATIENT['literacy']}")

    query = f"{PATIENT['drug']} discharge instructions"
    context = retrieve(index, chunks, query, drug=PATIENT["drug"], k=TOP_K)
    leaflet = make_leaflet(PATIENT, context)
    summary = summarize_leaflet(leaflet)

    results.append({
        "patient": PATIENT,
        "leaflet": leaflet,
        "summary": summary
    })

    print("\n--- Leaflet ---\n", leaflet)
    print("\n--- Summary ---\n", summary)

with open("final_patient_leaflets_clean.json", "w") as f:
    json.dump(results, f, indent=2)
print("All leaflets saved as final_patient_leaflets_clean.json.")

"""# Readability + Spanish + MP3"""

import textstat
from transformers import MarianMTModel, MarianTokenizer
from gtts import gTTS
from IPython.display import HTML, display, Audio

def en_to_es(text):
    """Translate English text to Spanish using MarianMT."""
    model = "Helsinki-NLP/opus-mt-en-es"
    tok = MarianTokenizer.from_pretrained(model)
    mdl = MarianMTModel.from_pretrained(model)
    ins = tok(text, return_tensors="pt", truncation=True)
    out = mdl.generate(**ins, max_new_tokens=800)
    return tok.batch_decode(out, skip_special_tokens=True)[0]

# Compute readability and translation
leaflet_en = leaflet.strip()
fk_grade = textstat.flesch_kincaid_grade(leaflet_en)
leaflet_es = en_to_es(leaflet_en) if leaflet_en else "No English text available."
audio_filename = "leaflet_es.mp3"

# Save Spanish audio narration
if leaflet_es and leaflet_es.strip():
    gTTS(leaflet_es, lang="es").save(audio_filename)

# Build HTML card (light/dark adaptive)
html = f"""
<style>
  :root {{
    --bg-light: #f8fafc;
    --bg-dark: #0f172a;
    --text-light: #111827;
    --text-dark: #f9fafb;
    --card-light: #ffffff;
    --card-dark: #1e293b;
    --accent-blue: #3b82f6;
    --accent-green: #22c55e;
  }}
  @media (prefers-color-scheme: dark) {{
    body {{ background: var(--bg-dark); color: var(--text-dark); }}
    .card {{ background: var(--card-dark); border: 1px solid #334155; }}
  }}
  @media (prefers-color-scheme: light) {{
    body {{ background: var(--bg-light); color: var(--text-light); }}
    .card {{ background: var(--card-light); border: 1px solid #e2e8f0; }}
  }}
  .card {{
    border-radius: 12px;
    padding: 18px 22px;
    max-width: 900px;
    margin: 20px auto;
    font-family: 'Inter', sans-serif;
    box-shadow: 0 4px 12px rgba(0,0,0,0.08);
  }}
  h2 {{
    color: var(--accent-blue);
    font-size: 1.4rem;
    margin-bottom: 0.5em;
  }}
  h3 {{
    color: var(--accent-green);
    font-size: 1.1rem;
    margin-top: 1.4em;
    margin-bottom: 0.4em;
  }}
  .section {{
    white-space: pre-wrap;
    line-height: 1.6;
    font-size: 0.95rem;
    padding: 12px 16px;
    border-radius: 8px;
  }}
  .section.en {{ background: rgba(59,130,246,0.05); }}
  .section.es {{ background: rgba(34,197,94,0.05); }}
  .footer {{
    font-size: 0.9rem;
    color: #94a3b8;
    margin-top: 20px;
    text-align: center;
  }}
</style>

<div class="card">
  <h2>Multimodal Patient Kit </h2>
  <p><b>Readability (F‚ÄìK grade):</b> {fk_grade if fk_grade else "N/A"}</p>

  <h3>Leaflet (English)</h3>
  <div class="section en">{leaflet_en[:4000]}</div>

  <h3>Leaflet (Spanish)</h3>
  <div class="section es">{leaflet_es[:4000]}</div>

  <div class="footer">
    <p>Audio narration generated: <code>{audio_filename}</code></p>
    <p><i>Educational demo only ‚Äî not medical advice.</i></p>
  </div>
</div>
"""

display(HTML(html))

# Only play audio if generated successfully
try:
    display(Audio(audio_filename, autoplay=False))
except Exception:
    print("Audio playback unavailable.")

"""Problem Statement:

	‚Ä¢	Health literacy gap: Many patients struggle to understand clinical drug labels or discharge instructions, which are written at a high reading level (often > 12th grade).
	‚Ä¢	Language barriers: Non-English speakers (e.g., Spanish-speaking patients in the US) often lack accessible instructions in their preferred language.
	‚Ä¢	Accessibility issues: Visually impaired or low-literacy patients need audio or simplified formats.
	‚Ä¢	Grounding & safety: Patients can get misinformation if instructions aren‚Äôt linked to authoritative sources like MedlinePlus or openFDA.


What This Project Solves:

	1.	Simplifies complex medical text
	‚Ä¢	Uses an open-source LLM to convert FDA drug labels & MedlinePlus guidelines into 8th-grade-level bullet points.
	‚Ä¢	Makes instructions easier to read, remember, and follow.
	2.	Bridges language barriers
	‚Ä¢	Provides automatic translation (e.g., English ‚Üí Spanish) so patients can access instructions in their native language.
	3.	Supports accessibility
	‚Ä¢	Converts the simplified leaflet into audio (MP3) for patients with low literacy or vision impairments.
	4.	Grounded in authoritative data
	‚Ä¢	Retrieves content from trusted medical sources (MedlinePlus & openFDA), ensuring safety and minimizing hallucinations.
	5.	Evaluation-ready
	‚Ä¢	Provides readability scores (Flesch‚ÄìKincaid) so clinicians/researchers can verify simplification effectiveness.

## Medicaid Policy Document Analysis
"""

from datasets import load_dataset
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np, re, json
import easyocr

import io, os, sys, re, json, subprocess
from pathlib import Path
from tqdm.auto import tqdm
from google.colab import auth, drive
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import fitz
from pdf2image import convert_from_path
from PIL import Image
import numpy as np
import easyocr

"""## Point to PDFs and Extract text using OCR"""

IN_COLAB = "google.colab" in sys.modules
if IN_COLAB:
    drive.mount("/content/drive")

auth.authenticate_user()
service = build("drive", "v3")

FOLDER_ID = "1fin1fkn-uxxS6D80ULOF0b4Q8Gva5N9A"
DEST_DIR = Path("/content/medicaid_ocr/raw_pdfs")
DEST_DIR.mkdir(parents=True, exist_ok=True)

def download_pdfs_from_folder(folder_id: str, dest_dir: Path):
    query = f"'{folder_id}' in parents and mimeType='application/pdf'"
    results = service.files().list(q=query, pageSize=500, fields="files(id, name)").execute()
    files = results.get("files", [])
    print(f"Found {len(files)} PDFs. Downloading to {dest_dir} ...")

    for f in tqdm(files, desc="Downloading PDFs"):
        request = service.files().get_media(fileId=f["id"])
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        out_path = dest_dir / f["name"]
        with open(out_path, "wb") as out:
            out.write(fh.getvalue())

download_pdfs_from_folder(FOLDER_ID, DEST_DIR)

"""##  OCR all PDFs ‚Üí one clean .txt per PDF"""

import os, re, json, subprocess
from pathlib import Path
from typing import List, Dict
from tqdm.auto import tqdm

import fitz                              # PyMuPDF
from pdf2image import convert_from_path  # poppler
import pytesseract                       # Tesseract OCR
from PIL import Image

BASE_DIR   = Path("/content/medicaid_ocr")
RAW_DIR    = BASE_DIR / "raw_pdfs"      # downloaded PDFs
TXT_DIR    = BASE_DIR / "txt"           # OCR output (.txt)
TMP_DIR    = BASE_DIR / "tmp"           # temp files
OUT_DIR    = Path("/content/data/processed")
for p in [TXT_DIR, TMP_DIR, OUT_DIR]: p.mkdir(parents=True, exist_ok=True)

DOCS_JSONL   = OUT_DIR / "docs.jsonl"
CHUNKS_JSONL = OUT_DIR / "chunks.jsonl"

def normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def fitz_extract_text(pdf_path: str) -> str:
    """Extract text layer using PyMuPDF."""
    pages = []
    with fitz.open(pdf_path) as doc:
        for page in doc:
            pages.append(page.get_text("text") or "")
    return normalize_spaces("\n\n".join(pages))

def ocrmypdf_available() -> bool:
    try:
        subprocess.run(["ocrmypdf", "--version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        return True
    except Exception:
        return False

def ocrmypdf_to_searchable(in_pdf: str, out_pdf: str) -> bool:
    cmd = [
        "ocrmypdf",
        "--skip-text", "--rotate-pages", "--deskew", "--clean",
        "--language","eng", "--output-type","pdfa",
        in_pdf, out_pdf
    ]
    try:
        r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
        return r.returncode == 0
    except Exception:
        return False

def ocr_via_images(in_pdf: str, dpi: int = 300) -> str:
    """Full image OCR fallback with Tesseract."""
    pages = convert_from_path(in_pdf, dpi=dpi)
    texts = []
    for img in pages:
        texts.append(pytesseract.image_to_string(img))
    return normalize_spaces("\n\n".join(texts))

def extract_text_resilient(in_pdf: str, tmp_dir: Path) -> str:
    """
    1) Try native text layer
    2) If thin/empty, try OCRmyPDF -> re-parse
    3) Else fall back to image OCR
    """
    # 1) native
    t1 = fitz_extract_text(in_pdf)
    if len(t1) > 500:
        return t1

    # 2) OCRmyPDF
    if ocrmypdf_available():
        out_pdf = tmp_dir / (Path(in_pdf).stem + "_ocr.pdf")
        ok = ocrmypdf_to_searchable(in_pdf, str(out_pdf))
        if ok and out_pdf.exists():
            t2 = fitz_extract_text(str(out_pdf))
            try: out_pdf.unlink()
            except: pass
            if len(t2) > max(200, len(t1)):
                return t2

    # 3) images
    return ocr_via_images(in_pdf)

def file_to_chunks(text: str, chunk_chars: int = 1200, overlap: int = 200) -> List[str]:
    """Fixed-size character chunks with overlap; skip tiny tail pieces."""
    text = normalize_spaces(text)
    if not text: return []
    chunks, step = [], max(1, chunk_chars - overlap)
    for i in range(0, len(text), step):
        ch = text[i:i+chunk_chars]
        if len(ch) < 200: break
        chunks.append(ch)
    return chunks

pdf_paths = sorted([p for p in RAW_DIR.glob("*.pdf") if p.is_file()])
print(f"Found {len(pdf_paths)} PDFs in {RAW_DIR}")

results: List[Dict] = []
for pdf in tqdm(pdf_paths, desc="OCR PDFs"):
    txt_path = TXT_DIR / f"{pdf.stem}.txt"
    if not txt_path.exists() or txt_path.stat().st_size == 0:
        try:
            text = extract_text_resilient(str(pdf), TMP_DIR)
            txt_path.write_text(text, encoding="utf-8")
        except Exception as e:
            print(f"[error] {pdf.name}: {e}")
            continue
    else:
        text = txt_path.read_text(encoding="utf-8", errors="ignore")
    results.append({"pdf": str(pdf), "txt": str(txt_path), "chars": len(text)})

print(f"OCR complete. Wrote {len(results)} txt files to {TXT_DIR}")

docs_rows, chunk_rows = [], []
for r in results:
    pdf_path = Path(r["pdf"])
    doc_id   = pdf_path.stem
    text     = Path(r["txt"]).read_text(encoding="utf-8", errors="ignore")

    # docs.jsonl (one row per PDF)
    docs_rows.append({
        "doc_id": doc_id,
        "source": "drive",
        "url": str(pdf_path),
        "title": doc_id,
        "section": None,
        "text": text
    })

    # chunks.jsonl (many rows per PDF)
    for i, ch in enumerate(file_to_chunks(text, chunk_chars=1200, overlap=200), 1):
        chunk_rows.append({
            "id": f"{doc_id}::chunk{i}",
            "doc_id": doc_id,
            "source": "drive",
            "url": str(pdf_path),
            "title": doc_id,
            "section": None,
            "text": ch
        })

with open(DOCS_JSONL, "w", encoding="utf-8") as f:
    for row in docs_rows:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

with open(CHUNKS_JSONL, "w", encoding="utf-8") as f:
    for row in chunk_rows:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

print(f"Wrote {len(docs_rows)} docs -> {DOCS_JSONL}")
print(f"Wrote {len(chunk_rows)} chunks -> {CHUNKS_JSONL}")

if docs_rows:
    print("\nExample doc:", docs_rows[0]["doc_id"])
    print("Doc snippet:", docs_rows[0]["text"][:200], "...")
if chunk_rows:
    print("\nExample chunk:", chunk_rows[0]["id"])
    print("Chunk snippet:", chunk_rows[0]["text"][:200], "...")

"""## Build a searchable index (BGE + FAISS)"""

!pip -q install faiss-cpu sentence-transformers

from pathlib import Path
import json, os, numpy as np, faiss
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm

CHUNKS_JSONL = "/content/data/processed/chunks.jsonl"
INDEX_DIR     = Path("/content/data/index")
INDEX_DIR.mkdir(parents=True, exist_ok=True)

EMB_MODEL = "BAAI/bge-small-en-v1.5"
# alternatives:
# EMB_MODEL = "BAAI/bge-base-en-v1.5"
# EMB_MODEL = "BAAI/bge-large-en-v1.5"

model = SentenceTransformer(EMB_MODEL)

# BGE uses instruction prefixes:
QUERY_PREFIX    = "Represent this query for retrieving relevant documents: "
PASSAGE_PREFIX  = "Represent this passage for retrieval: "

def embed_passages(texts, batch_size=256, show_progress=True):
    # prepend passage instruction
    texts_ = [PASSAGE_PREFIX + (t or "") for t in texts]
    X = model.encode(
        texts_,
        batch_size=batch_size,
        show_progress_bar=show_progress,
        convert_to_numpy=True,
        normalize_embeddings=True,   # L2 normalize for cosine/IP
    ).astype("float32")
    return X

def embed_queries(queries):
    qs = [QUERY_PREFIX + q for q in queries]
    Q = model.encode(
        qs,
        batch_size=64,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True,
    ).astype("float32")
    return Q

# Load chunks
chunks = [json.loads(l) for l in open(CHUNKS_JSONL, "r", encoding="utf-8")]
texts  = [c["text"] for c in chunks]

# Embed
X = embed_passages(texts, batch_size=256, show_progress=True)  # shape [N, d]
d = X.shape[1]

index = faiss.IndexFlatIP(d)
index.add(X)

# Persist index + metadata
faiss.write_index(index, str(INDEX_DIR / "faiss.index"))
with open(INDEX_DIR / "meta.jsonl", "w", encoding="utf-8") as f:
    for row in chunks:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

np.save(INDEX_DIR / "embeddings.npy", X)

print(f"Built FAISS. vectors={index.ntotal}, dim={d}")
print("Saved to:", INDEX_DIR)

# HNSW variant (approximate nearest neighbors)
M = 32
ef_construction = 200
index_hnsw = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)
index_hnsw.hnsw.efConstruction = ef_construction
index_hnsw.add(X)
faiss.write_index(index_hnsw, str(INDEX_DIR / "faiss_hnsw.index"))
print("HNSW index saved.")

# Load utilities
def load_faiss(path=INDEX_DIR / "faiss.index", meta_path=INDEX_DIR / "meta.jsonl"):
    idx = faiss.read_index(str(path))
    meta = [json.loads(l) for l in open(meta_path, "r", encoding="utf-8")]
    return idx, meta

def search(query, top_k=8, use_hnsw=False):
    idx_path = INDEX_DIR / ("faiss_hnsw.index" if use_hnsw else "faiss.index")
    index, meta = load_faiss(idx_path, INDEX_DIR / "meta.jsonl")
    Q = embed_queries([query])  # [1, d]
    scores, idxs = index.search(Q, top_k)  # scores: [1,k], idxs: [1,k]
    idxs = idxs[0].tolist(); scores = scores[0].tolist()
    hits = []
    for rank, (i, s) in enumerate(zip(idxs, scores), 1):
        if i == -1:  # FAISS returns -1 if not enough vectors
            continue
        m = meta[i]
        hits.append({
            "rank": rank,
            "score": float(s),
            "id": m.get("id"),
            "doc_id": m.get("doc_id"),
            "title": m.get("title"),
            "url": m.get("url"),
            "preview": (m.get("text") or "")[:220].replace("\n", " ")
        })
    return hits

# quick smoke test
for h in search("copay exemptions for NY Medicaid pharmacy", top_k=5):
    print(f"{h['rank']:>2}. score={h['score']:.3f} | {h['doc_id']} | {h['preview']}")

import torch
from sentence_transformers import CrossEncoder
import numpy as np
from typing import List, Dict

# pick one:
RERANKER_MODEL = "BAAI/bge-reranker-base"   # good balance
# RERANKER_MODEL = "BAAI/bge-reranker-large"  # strongest, ~1.3GB
# RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # very fast

device = "cuda" if torch.cuda.is_available() else "cpu"
reranker = CrossEncoder(RERANKER_MODEL, max_length=512, device=device)

def rerank_hits(query: str,
                hits: List[Dict],
                text_key: str = "text",
                batch_size: int = 64,
                combine_with_dense: bool = False,
                alpha: float = 0.5) -> List[Dict]:
    """
    hits: list of dicts with at least {text_key, score} where 'score' is your dense score.
    If combine_with_dense=True, we z-score both dense and rerank scores and blend.
    Otherwise we sort by rerank score only (most common).
    """
    if not hits:
        return hits

    # prepare pairs for cross-encoder
    pairs = [(query, h[text_key]) for h in hits]

    # batched prediction (saves memory)
    scores = []
    for i in range(0, len(pairs), batch_size):
        batch = pairs[i:i+batch_size]
        with torch.inference_mode():
            s = reranker.predict(batch)
        # ensure python floats
        scores.extend([float(x) for x in (s.tolist() if hasattr(s, "tolist") else s)])

    # attach rerank score
    for h, s in zip(hits, scores):
        h["rerank_score"] = s

    if combine_with_dense:
        # z-score both, then blend
        ds = np.array([h.get("score", 0.0) for h in hits], dtype=np.float32)
        rs = np.array([h["rerank_score"] for h in hits], dtype=np.float32)

        def z(x):
            mu, sd = float(x.mean()), float(x.std() + 1e-6)
            return (x - mu) / sd

        final = alpha * z(ds) + (1.0 - alpha) * z(rs)
        for h, f in zip(hits, final):
            h["final_score"] = float(f)
        hits.sort(key=lambda x: x["final_score"], reverse=True)
    else:
        # typical: order by cross-encoder score only
        hits.sort(key=lambda x: x["rerank_score"], reverse=True)

    # reassign ranks
    for i, h in enumerate(hits, 1):
        h["rank"] = i
    return hits

import faiss, json
from pathlib import Path

INDEX_DIR = Path("/content/data/index")
FAISS_PATH = INDEX_DIR / "faiss.index"
META_PATH  = INDEX_DIR / "meta.jsonl"

# load FAISS + meta
faiss_index = faiss.read_index(str(FAISS_PATH))
meta = [json.loads(l) for l in open(META_PATH, "r", encoding="utf-8")]

def dense_search_with_text(query: str, top_k: int = 50) -> List[Dict]:
    """Dense search (BGE) that returns dicts with text + dense score."""
    Q = embed_queries([query])  # from your BGE code
    scores, idxs = faiss_index.search(Q, top_k)
    idxs = idxs[0].tolist(); scores = scores[0].tolist()
    hits = []
    for rank, (i, s) in enumerate(zip(idxs, scores), 1):
        if i == -1:  # not enough vectors
            continue
        m = meta[i]
        hits.append({
            "rank": rank,
            "score": float(s),        # dense score (cosine/IP)
            "id": m.get("id"),
            "doc_id": m.get("doc_id"),
            "title": m.get("title"),
            "url": m.get("url"),
            "text": m.get("text", "")[:8192]  # keep within CE max_length context
        })
    return hits

def retrieve_reranked(query: str, top_k_candidates: int = 50, top_k_final: int = 8,
                      combine_with_dense: bool = False, alpha: float = 0.5) -> List[Dict]:
    prelim = dense_search_with_text(query, top_k=top_k_candidates)
    reranked = rerank_hits(query, prelim, text_key="text",
                           batch_size=64,
                           combine_with_dense=combine_with_dense,
                           alpha=alpha)
    return reranked[:top_k_final]

q = "copay exemptions for NY Medicaid pharmacy claims"
hits = retrieve_reranked(q, top_k_candidates=50, top_k_final=8, combine_with_dense=False)

for h in hits:
    print(f"{h['rank']:>2}. rr={h['rerank_score']:.3f}  d={h['score']:.3f}  {h['doc_id']}  {h['text'][:140].replace('\n',' ')}")

"""## Search + generate policy bullets (LLM)"""

# ==== Generator + grounded answer using RE-RANKED hits (no 4-bit needed) ====
!pip -q install transformers accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_CANDIDATES = [
    "Qwen/Qwen2.5-1.5B-Instruct",     # small, good quality
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
]

def load_llm_no_quant():
    last_err = None
    for model_id in MODEL_CANDIDATES:
        try:
            tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            mdl = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            )
            return pipeline(
                "text-generation",
                model=mdl,
                tokenizer=tok,
                max_new_tokens=700,
                temperature=0.2,
                do_sample=True,
                pad_token_id=tok.eos_token_id,
            )
        except Exception as e:
            print(f"[warn] failed loading {model_id}: {e}")
            last_err = e
    raise RuntimeError(f"Could not load any model; last error: {last_err}")

gen = load_llm_no_quant()

def build_context_blocks(hits):
    blocks, tag_map = [], []
    for i, h in enumerate(hits, 1):
        tag  = f"[DOC:{i}]"
        head = f"{tag} {h.get('title') or h.get('doc_id') or ''} ‚Äî {h.get('url','')}"
        body = (h.get('text') or '').strip()
        blocks.append(head + "\n" + body)
        tag_map.append({"tag": tag, "doc_id": h.get("doc_id"), "url": h.get("url")})
    return "\n\n".join(blocks), tag_map

SYSTEM_MSG = (
    "You are a Medicaid policy assistant. Use ONLY the provided Sources. "
    "Answer as concise bullet points. Each bullet MUST end with one citation tag like [DOC:1]. "
    "If an answer is not present in Sources, write: 'Not in provided context.'"
)

def policy_answer(query: str, k_candidates: int = 50, k_final: int = 8,
                  combine_with_dense: bool = False, alpha: float = 0.5):
    # Retrieve with your cross-encoder reranker
    hits = retrieve_reranked(
        query,
        top_k_candidates=k_candidates,
        top_k_final=k_final,
        combine_with_dense=combine_with_dense,
        alpha=alpha
    )
    context, tag_map = build_context_blocks(hits)
    prompt = (
        f"<s>[SYSTEM] {SYSTEM_MSG}\n"
        f"[SOURCES]\n{context}\n\n"
        f"[USER]\n{query}\n"
        f"[ASSISTANT]"
    )
    out = gen(prompt)[0]["generated_text"]
    answer = out.split("[ASSISTANT]")[-1].strip()
    return answer, hits, tag_map

# quick test
q = "Summarize NYRx copay exemptions applicable to pharmacy claims."
ans, used_hits, tags = policy_answer(q, k_candidates=50, k_final=8)
print("‚Äî Answer ‚Äî\n", ans, "\n")
print("‚Äî Sources ‚Äî")
for i, h in enumerate(used_hits, 1):
    print(f"{i:>2}. rr={h.get('rerank_score',0):.3f}  d={h.get('score',0):.3f}  {h.get('doc_id')}  {h.get('url')}")

def policy_sidebar_for_drug(drug_name: str):
    q = (f"For {drug_name}: summarize any Medicaid (NY) pharmacy coverage notes, NYRx billing rules, "
         f"prior auth/transition fills, 340B markers, copay or eligibility updates relevant to counseling a patient.")
    answer, hits, _ = policy_answer(q, k_candidates=50, k_final=8)
    return answer, hits

import pandas as pd
from textwrap import shorten

def hits_table(hits):
    # hits come from retrieve_reranked ‚Üí include rerank_score & dense score
    rows = []
    for h in hits:
        rows.append({
            "rank": h.get("rank"),
            "rerank_score": round(h.get("rerank_score", 0.0), 3),
            "dense_score":  round(h.get("score", 0.0), 3),
            "doc_id": h.get("doc_id"),
            "url": h.get("url"),
            "preview": shorten((h.get("text") or "").replace("\n"," "), width=160, placeholder=" ...")
        })
    return pd.DataFrame(rows)

def show_top_snippets(hits, n=3, max_chars=300):
    print("\nTop context snippets:")
    for h in hits[:n]:
        preview = shorten((h.get("text") or "").replace("\n"," "), width=max_chars, placeholder=" ...")
        print(f"  [DOC:{h.get('rank')}]  rr={h.get('rerank_score',0):.3f} d={h.get('score',0):.3f}  {h.get('doc_id')}\n   {preview}\n")

def ask_and_print(query, k_candidates=50, k_final=8, save_prefix=None, combine_with_dense=False, alpha=0.5):
    print("\n" + "="*100)
    print("QUERY:", query)

    # policy_answer uses retrieve_reranked under the hood
    answer, hits, _ = policy_answer(
        query,
        k_candidates=k_candidates,
        k_final=k_final,
        combine_with_dense=combine_with_dense,
        alpha=alpha
    )

    print("\n--- LLM Answer (grounded) ---\n")
    print(answer)

    # Citations / sources table
    df = hits_table(hits)
    print("\n--- Sources (re-ranked) ---")
    display(df)

    # Longer console previews
    show_top_snippets(hits, n=3, max_chars=320)

    # Optional: save artifacts
    if save_prefix:
        df.to_csv(f"{save_prefix}_sources.csv", index=False)
        with open(f"{save_prefix}_answer.txt","w") as f:
            f.write(answer)
        print(f"\nSaved: {save_prefix}_answer.txt and {save_prefix}_sources.csv")

queries = [
    "Summarize NYRx pharmacy benefit transition guidance for providers and pharmacies.",
    "What do the Medicaid Updates say about 340B claim identifiers or ceiling price rules in 2024?",
    "List any Medicaid copay exemptions or eligibility changes that impact pharmacy claims.",
    "What are the instructions for NCPDP billing or MEVS/DVS checks mentioned in these bulletins?",
]

for i, q in enumerate(queries, 1):
    ask_and_print(q, k_candidates=50, k_final=8, save_prefix=f"qa_run_{i}")

queries = [
    # New extended queries
    "When did the NYRx transition from managed care to fee-for-service occur?",
    "What are the Medicaid prior authorization requirements for long-acting injectables?",
    "What documentation must pharmacies keep for 340B claims?",
    "Which medications require Dispense As Written (DAW) 1 to ensure brand coverage?",
    "What are the Medicaid guidelines for emergency contraception coverage?",
    "What telehealth services are covered under the Medicaid model contract?",
    "How should pharmacies bill Medicaid for over-the-counter medications?",
    "What is the reimbursement policy for compound drugs under NYRx?",
    "What are the guidelines for billing vaccines administered in pharmacies?",
    "What are the Medicaid rules for copayments for members under 21?",
    "Which services are covered under the Health Home program in the NYHER Waiver?",
    "When did the last Medicaid pharmacy update revise the DUR criteria?",
    "What are the requirements for appointment scheduling in the Medicaid model contract for urgent care?",
    "When did the pharmacy carve out occur?",
    "What are the key components of the SCN program in the NYHER Waiver?",
    "What constitutes RRP referral requirements?",
    "What are the requirements for a referral for enrollment in the childrens waiver?",
    "What are REC services offered to NYS providers?"
]

for i, q in enumerate(queries, 1):
    ask_and_print(q, k_candidates=50, k_final=8, save_prefix=f"qa_run_{i}")

!pip -q install gradio

import gradio as gr
import pandas as pd
from datetime import datetime

def _make_df(hits, k_final):
    # use your hits_table if present; otherwise fall back to a compact table
    try:
        return hits_table(hits).head(k_final)
    except NameError:
        rows = []
        for i, h in enumerate(hits[:k_final]):
            rows.append({
                "rank": h.get("rank", i+1),
                "rerank_score": round(h.get("rerank_score", h.get("score", 0.0)), 3),
                "dense_score": round(h.get("score", 0.0), 3),
                "doc_id": h.get("doc_id", ""),
                "url": h.get("url", ""),
                "preview": (h.get("text") or "").replace("\n"," ")[:280]
            })
        return pd.DataFrame(rows)

def ui_policy_answer(query, k_candidates, k_final, alpha, combine_with_dense):
    if not query.strip():
        return "Please enter a question.", pd.DataFrame(), None
    answer, hits, _ = policy_answer(
        query=query,
        k_candidates=k_candidates,
        k_final=k_final,
        combine_with_dense=combine_with_dense,
        alpha=alpha
    )
    df = _make_df(hits, k_final)
    # small download artifact
    fname = f"policy_answer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(fname, "w") as f:
        f.write(answer + "\n\n--- Sources ---\n")
        for i, h in enumerate(hits[:k_final], 1):
            f.write(f"[{i}] {h.get('doc_id','')} {h.get('url','')}\n")
    return answer, df, fname

with gr.Blocks(title="Medicaid Policy Assistant") as demo:
    gr.Markdown("## Medicaid Policy Assistant\nAsk a question; the system retrieves, reranks, and answers with sources.")
    q = gr.Textbox(label="Your question", lines=2, placeholder="e.g., What do the Medicaid Updates say about 340B claim identifiers?")
    with gr.Row():
        k1 = gr.Slider(10, 200, value=50, step=10, label="k_candidates")
        k2 = gr.Slider(3, 20, value=8, step=1, label="k_final")
        a  = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="alpha (dense‚Üîcross blend)")
        cwb = gr.Checkbox(value=False, label="Combine with dense")
    run = gr.Button("Run", variant="primary")
    ans = gr.Textbox(label="Answer", lines=8)
    table = gr.Dataframe(label="Top sources", wrap=True, interactive=False)
    dl = gr.File(label="Download answer + sources")

    run.click(ui_policy_answer, inputs=[q, k1, k2, a, cwb], outputs=[ans, table, dl])

demo.launch(share=True)

